@book{Davies.2008cop.2006,
  year      = {2008, cop. 2006},
  title     = {Semantic web technologies: Trends and research in ontology-based systems},
  address   = {Chichester},
  edition   = {Reprinted.},
  publisher = {{J. Wiley {\&} Sons}},
  isbn      = {978-0-470-02596-3},
  editor    = {Davies, J. and Studer, Rudi and Warren, Paul},
  file      = {Davies, Studer et al (Ed) 2008, cop 2006 - Semantic web technologies:Attachments/Davies, Studer et al (Ed) 2008, cop 2006 - Semantic web technologies.pdf:application/pdf}
}


@misc{DuXinya.29042017,
  abstract = {We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).},
  author   = {{Du Xinya} and Shao, Junru and Cardie, Claire},
  date     = {29/04/2017},
  title    = {Learning to Ask: Neural Question Generation for Reading Comprehension},
  url      = {https://arxiv.org/pdf/1705.00106.pdf},
  urldate  = {28/05/2023},
  year     = {2017},
  file     = {1705.00106:Attachments/1705.00106.pdf:application/pdf}
}

@online{kdnug,
  author       = {Matthew Mayo},
  title        = {{Building a Wikipedia Text Corpus for Natural Language Processing}},
  howpublished = {\url{https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html}},
  year         = {2017},
  note         = {[Online; accessed 01-May-2023]}
}

@thesis{LisaEhrlinger,
  abstract = {Recently, the term knowledge graph has been used frequently in research and business, usually in close association with Semantic Web technologies, linked data, large-scale data analytics and cloud computing. Its popularity is clearly influenced by the introduction of Google’s Knowledge Graph in 2012, and since then the term has been widely used without a definition. A large variety of interpretations has hampered the evolution of a common understanding of knowledge graphs. Numerous research papers refer to Google’s Knowledge Graph, although no official documentation about the used methods exists. The prerequisite for widespread academic and commercial adoption of a concept or technology is a common understanding, based ideally on a definition that is free from ambiguity. We tackle this issue by discussing and defining the term knowledge graph, considering its history and diversity in interpretations and use. Our goal is to propose a definition of knowledge graphs that serves as basis for discussions on this topic and contributes to a common vision.},
  author   = {Lisa Ehrlinger, Wolfram Wöß},
  title    = {Towards a Definition of Knowledge Graphs},
  year     = {2016},
  keywords = {Knowledge Graphs}
}

% This file was created with Citavi 6.15.2.0

@article{LupeHernandez,
  title   = {Question Generator
             Natural Language Processing},
  author  = {Lupe Hernandez and Sam Randall, Ahmad Nazeri},
  url     = {http://cs230.stanford.edu/projects_fall_2020/reports/55771015.pdf},
  urldate = {29/05/2023},
  year    = {2020},
  file    = {55771015:Attachments/55771015.pdf:application/pdf}
}


@InProceedings{ mckinney-proc-scipy-2010,
  author    = { {W}es {M}c{K}inney },
  title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
  booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
  pages     = { 56 - 61 },
  year      = { 2010 },
  editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
  doi       = { 10.25080/Majora-92bf1922-00a }
}






% This file was created with Citavi 6.15.2.0

@article{PranavRajpurkar.,
  abstract = {EMNLP2016 2016},
  author   = {{Pranav Rajpurkar} and {Jian Zhang} and {Konstantin Lopyrev} and {Percy Liang}},
  title    = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  url      = {https://aclanthology.org/D16-1264.pdf},
  urldate  = {29/05/2023},
  year     = 2016,
  file     = {100,000  Questions for Machine Comprehension of Text:Attachments/100,000  Questions for Machine Comprehension of Text.pdf:application/pdf}
}

@inproceedings{rehurek_lrec,
  title     = {{Software Framework for Topic Modelling with Large Corpora}},
  author    = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
  booktitle = {{Proceedings of the LREC 2010 Workshop on New
               Challenges for NLP Frameworks}},
  pages     = {45--50},
  year      = 2010,
  month     = May,
  day       = 22,
  publisher = {ELRA},
  address   = {Valletta, Malta},
  language  = {English}
}

@inproceedings{SciPyProceedings_11,
  author    = {Aric A. Hagberg and Daniel A. Schult and Pieter J. Swart},
  title     = {Exploring Network Structure, Dynamics, and Function using NetworkX},
  booktitle = {Proceedings of the 7th Python in Science Conference},
  pages     = {11 - 15},
  address   = {Pasadena, CA USA},
  year      = {2008},
  editor    = {Ga\"el Varoquaux and Travis Vaught and Jarrod Millman}
}

% This file was created with Citavi 6.15.2.0

@unpublished{spacy2,
  author = {Honnibal, Matthew and Montani, Ines},
  title  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
  year   = {2017},
  note   = {spaCy excels at large-scale information extraction tasks.}
}


% This file was created with Citavi 6.15.2.0

@phdthesis{Sukhbaatar.31032015,
  abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
  author   = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
  year     = {31/03/2015},
  title    = {End-To-End Memory Networks},
  keywords = {Computer Science - Computation and Language;Computer Science - Neural and Evolutionary Computing},
  file     = {1503.08895:Attachments/1503.08895.pdf:application/pdf}
}



% This file was created with Citavi 6.15.2.0

@article{TorstenZesch,
  title   = {Analyzing and Accessing Wikipedia as a Lexical Semantic Resource},
  author  = {Torsten Zesch, Iryna Gurevych, Max Mühlhäuser},
  year    = {2008},
}





@online{wikiapi,
  year  = {2023},
  title = {Wikipedia API documentation},
  url   = {https://wikipedia-api.readthedocs.io/en/latest/wikipediaapi/api.html}
}




@online{wkpd,
  author       = {Wikipedia},
  title        = {{Wikipedia special exports}},
  howpublished = {\url{https://en.wikipedia.org/wiki/Special:Export}},
  year         = {2023},
  note         = {[Online; accessed 01-May-2023]}
}


@techreport{hagberg2008exploring,
  title={Exploring network structure, dynamics, and function using NetworkX},
  author={Hagberg, Aric and Swart, Pieter and S Chult, Daniel},
  year={2008},
  institution={Los Alamos National Lab.(LANL), Los Alamos, NM (United States)}
}

@inproceedings{mckinney2010data,
  title={Data structures for statistical computing in python},
  author={McKinney, Wes and others},
  booktitle={Proceedings of the 9th Python in Science Conference},
  volume={445},
  pages={51--56},
  year={2010},
  organization={Austin, TX}
}


@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

% This file was created with Citavi 6.15.2.0

@misc{Paszke.03122019,
 abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.  In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.  We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 year = {2019},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://arxiv.org/pdf/1912.01703.pdf},
 file = {1912.01703:Attachments/1912.01703.pdf:application/pdf}
}


@online{sean,
  author       = {Sean Robertson},
  title        = {{Classifying names with a character level RNN}},
  url={https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html},
  year         = {2023},
  urldate         = {02-07-2023},
}
