@book{Davies.2008cop.2006,
  year      = {2008, cop. 2006},
  title     = {Semantic web technologies: Trends and research in ontology-based systems},
  address   = {Chichester},
  edition   = {Reprinted.},
  publisher = {{J. Wiley {\&} Sons}},
  isbn      = {978-0-470-02596-3},
  editor    = {Davies, J. and Studer, Rudi and Warren, Paul},
  file      = {Davies, Studer et al (Ed) 2008, cop 2006 - Semantic web technologies:Attachments/Davies, Studer et al (Ed) 2008, cop 2006 - Semantic web technologies.pdf:application/pdf}
}


@misc{DuXinya.29042017,
  abstract = {We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).},
  author   = {{Du Xinya} and Shao, Junru and Cardie, Claire},
  date     = {29/04/2017},
  title    = {Learning to Ask: Neural Question Generation for Reading Comprehension},
  url      = {https://arxiv.org/pdf/1705.00106.pdf},
  urldate  = {28/05/2023},
  year     = {2017},
  file     = {1705.00106:Attachments/1705.00106.pdf:application/pdf}
}

@online{kdnug,
  author       = {Matthew Mayo},
  title        = {{Building a Wikipedia Text Corpus for Natural Language Processing}},
  howpublished = {\url{https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html}},
  year         = {2017},
  note         = {[Online; accessed 01-May-2023]}
}

@thesis{LisaEhrlinger,
  abstract = {Recently, the term knowledge graph has been used frequently in research and business, usually in close association with Semantic Web technologies, linked data, large-scale data analytics and cloud computing. Its popularity is clearly influenced by the introduction of Google’s Knowledge Graph in 2012, and since then the term has been widely used without a definition. A large variety of interpretations has hampered the evolution of a common understanding of knowledge graphs. Numerous research papers refer to Google’s Knowledge Graph, although no official documentation about the used methods exists. The prerequisite for widespread academic and commercial adoption of a concept or technology is a common understanding, based ideally on a definition that is free from ambiguity. We tackle this issue by discussing and defining the term knowledge graph, considering its history and diversity in interpretations and use. Our goal is to propose a definition of knowledge graphs that serves as basis for discussions on this topic and contributes to a common vision.},
  author   = {Lisa Ehrlinger, Wolfram Wöß},
  title    = {Towards a Definition of Knowledge Graphs},
  year     = {2016},
  keywords = {Knowledge Graphs}
}

% This file was created with Citavi 6.15.2.0

@article{LupeHernandez,
  title   = {Question Generator
             Natural Language Processing},
  author  = {Lupe Hernandez and Sam Randall, Ahmad Nazeri},
  url     = {http://cs230.stanford.edu/projects_fall_2020/reports/55771015.pdf},
  urldate = {29/05/2023},
  year    = {2020},
  file    = {55771015:Attachments/55771015.pdf:application/pdf}
}

%pandas
@InProceedings{mckinney-proc-scipy-2010,
  author    = { {W}es {M}c{K}inney },
  title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
  booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
  pages     = { 56 - 61 },
  year      = { 2010 },
  editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
  doi       = { 10.25080/Majora-92bf1922-00a }
}






% This file was created with Citavi 6.15.2.0

@article{PranavRajpurkar.,
  abstract = {EMNLP2016 2016},
  author   = {{Pranav Rajpurkar} and {Jian Zhang} and {Konstantin Lopyrev} and {Percy Liang}},
  title    = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  url      = {https://aclanthology.org/D16-1264.pdf},
  urldate  = {29/05/2023},
  year     = 2016,
  file     = {100,000  Questions for Machine Comprehension of Text:Attachments/100,000  Questions for Machine Comprehension of Text.pdf:application/pdf}
}

@inproceedings{rehurek_lrec,
  title     = {{Software Framework for Topic Modelling with Large Corpora}},
  author    = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
  booktitle = {{Proceedings of the LREC 2010 Workshop on New
               Challenges for NLP Frameworks}},
  pages     = {45--50},
  year      = 2010,
  month     = May,
  day       = 22,
  publisher = {ELRA},
  address   = {Valletta, Malta},
  language  = {English}
}

@inproceedings{SciPyProceedings_11,
  author    = {Aric A. Hagberg and Daniel A. Schult and Pieter J. Swart},
  title     = {Exploring Network Structure, Dynamics, and Function using NetworkX},
  booktitle = {Proceedings of the 7th Python in Science Conference},
  pages     = {11 - 15},
  address   = {Pasadena, CA USA},
  year      = {2008},
  editor    = {Ga\"el Varoquaux and Travis Vaught and Jarrod Millman}
}

% This file was created with Citavi 6.15.2.0

@unpublished{spacy2,
  author = {Honnibal, Matthew and Montani, Ines},
  title  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
  year   = {2017},
  note   = {spaCy excels at large-scale information extraction tasks.}
}


% This file was created with Citavi 6.15.2.0

@phdthesis{Sukhbaatar.31032015,
  abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
  author   = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
  year     = {31/03/2015},
  title    = {End-To-End Memory Networks},
  keywords = {Computer Science - Computation and Language;Computer Science - Neural and Evolutionary Computing},
  file     = {1503.08895:Attachments/1503.08895.pdf:application/pdf}
}



% This file was created with Citavi 6.15.2.0

@article{TorstenZesch,
  title   = {Analyzing and Accessing Wikipedia as a Lexical Semantic Resource},
  author  = {Torsten Zesch, Iryna Gurevych, Max Mühlhäuser},
  year    = {2008},
}





@online{wikiapi,
  year  = {2023},
  title = {Wikipedia API documentation},
  url   = {https://wikipedia-api.readthedocs.io/en/latest/wikipediaapi/api.html}
}




@online{wkpd,
  author       = {Wikipedia},
  title        = {{Wikipedia special exports}},
  howpublished = {\url{https://en.wikipedia.org/wiki/Special:Export}},
  year         = {2023},
  note         = {[Online; accessed 01-May-2023]}
}


@techreport{hagberg2008exploring,
  title={Exploring network structure, dynamics, and function using NetworkX},
  author={Hagberg, Aric and Swart, Pieter and S Chult, Daniel},
  year={2008},
  institution={Los Alamos National Lab.(LANL), Los Alamos, NM (United States)}
}

@inproceedings{mckinney2010data,
  title={Data structures for statistical computing in python},
  author={McKinney, Wes and others},
  booktitle={Proceedings of the 9th Python in Science Conference},
  volume={445},
  pages={51--56},
  year={2010},
  organization={Austin, TX}
}


@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

% This file was created with Citavi 6.15.2.0

@misc{Paszke.03122019,
 abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.  In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.  We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 year = {2019},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://arxiv.org/pdf/1912.01703.pdf},
 file = {1912.01703:Attachments/1912.01703.pdf:application/pdf}
}


@online{sean,
  author       = {Sean Robertson},
  title        = {{Classifying names with a character level RNN}},
  url={https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html},
  year         = {2023},
 note         = {Accessed on 2023-05-1}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    url={http://www.deeplearningbook.org},
    year={2016},
    note={Accessed on 15-05-2023}
}

% This file was created with Citavi 6.15.2.0

@article{Rumelhart.1986,
 abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
 author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
 year = {1986},
 title = {Learning representations by back-propagating errors},
 pages = {533--536},
 volume = {323},
 number = {6088},
 issn = {1476-4687},
 journal = {Nature},
 doi = {10.1038/323533a0}
}


% This file was created with Citavi 6.15.2.0

@book{Book-Bishop-Neural,
 title = {Book-Bishop-Neural Networks for Pattern Recognition},
 publisher={Clarendon Press},
 author= {Christopher m. Bishop},
 year= {1995},
  note={Accessed on 01-05-2023}

}

% This file was created with Citavi 6.15.2.0

@misc{Grave.14092016,
 abstract = {We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further reduces the computational time by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at https://github.com/facebookresearch/adaptive-softmax.},
 author = {Grave, Edouard and Joulin, Armand and Ciss{\'e}, Moustapha and Grangier, David and J{\'e}gou, Herv{\'e}},
 date = {14/09/2016},
 title = {Efficient softmax approximation for GPUs},
 url = {https://arxiv.org/pdf/1609.04309.pdf},
 urldate = {08/02/2023},
 year         = {2016},

}


% This file was created with Citavi 6.15.2.0

@misc{Grave.14092016,
 abstract = {We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further reduces the computational time by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at https://github.com/facebookresearch/adaptive-softmax.},
 author = {Grave, Edouard and Joulin, Armand and Ciss{\'e}, Moustapha and Grangier, David and J{\'e}gou, Herv{\'e}},
 date = {14/09/2016},
 title = {Efficient softmax approximation for GPUs},
 url = {https://arxiv.org/pdf/1609.04309.pdf},
 urldate = {08/02/2023},
 year         = {2016},

}


@article{zhang2021dive,
    title={Dive into Deep Learning},
    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    journal={arXiv preprint arXiv:2106.11342},
    year={2021}
}


@article{cauchy,
    title={Gradient decent},
    author={Augustin-Louis Cauchy},
    year={1847}
}

% This file was created with Citavi 6.15.2.0

@misc{Smith.03062015,
 abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate {\textquotedbl}reasonable bounds{\textquotedbl} -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
 author = {Smith, Leslie N.},
 date = {03/06/2015},
  year = {2015},
 title = {Cyclical Learning Rates for Training Neural Networks}
}


@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}


% This file was created with Citavi 6.15.2.0

@misc{Gupta.20062016,
 abstract = {Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction. To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilizing (with respect to the taxonomy tree) a path-wise, node-wise and depth-wise classifiers for error reduction in the final product classification. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve better results on various evaluation metrics compared to earlier approaches.},
 author = {Gupta, Vivek and Karnick, Harish and Bansal, Ashendra and Jhala, Pradhuman},
 date = {20/06/2016},
year = {2016},
 title = {Product Classification in E-Commerce using Distributional Semantics},
 url = {https://arxiv.org/pdf/1606.06083.pdf},
 urldate = {09/03/2023},
 file = {1606.06083:Attachments/1606.06083.pdf:application/pdf}
}

% This file was created with Citavi 6.15.2.0

@online{AliCevahir.,
 author = {{Ali Cevahir} and {Koji Murakami}},
 title = {Large-scale Multi-class and Hierarchical Product Categorization for an E-commerce Giant},
 url = {https://aclanthology.org/C16-1051.pdf},
 year ={2016},
 urldate = {31/05/2023},
 file = {Ali Cevahir, Koji Murakami - Large-scale Multi-class and Hierarchical Product:Attachments/Ali Cevahir, Koji Murakami - Large-scale Multi-class and Hierarchical Product.pdf:application/pdf},
 note={Accessed on 01-05-2023}
}



% This file was created with Citavi 6.15.2.0

@book{BirdKleinLoper09,
  abstract = {This book offers a highly accessible introduction to Natural Language Processing, the field that underpins a variety of language technologies ranging from predictive text and email filtering to automatic summarization and translation. Using NLTK, you'll learn how to write Python programs to analyze the structure and meaning of texts, drawing on techniques from the fields of linguistics and artificial intelligence.},
  added-at = {2016-12-06T16:29:36.000+0100},
  address = {Beijing},
  author = {Bird, Steven and Klein, Ewan and Loper, Edward},
  biburl = {https://www.bibsonomy.org/bibtex/2c90dc59441d01c8bef58a947274164d4/flint63},
  doi = {http://my.safaribooksonline.com/9780596516499},
  file = {O'Reilly eBook:2009/BirdKleinLoper09.pdf:PDF;O'Reilly Product page:http\://shop.oreilly.com/product/9780596516499.do:URL;Related Web Site:http\://www.nltk.org/:URL;Safari:https\://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/:URL},
  groups = {public},
  interhash = {5408d7da097b9cd81239c238da8bfaf4},
  intrahash = {c90dc59441d01c8bef58a947274164d4},
  isbn = {978-0-596-51649-9},
  keywords = {01841 103 book safari ai software development language processing text python framework},
  publisher = {O'Reilly},
  timestamp = {2018-04-16T12:35:20.000+0200},
  title = {Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit},
  url = {http://www.nltk.org/book},
  username = {flint63},
  year = 2009,
   note={Accessed on 10-05-2023}
}


% This file was created with Citavi 6.15.2.0

@misc{Vaswani.12062017,
 abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
 date = {12/06/2017},
 year = {2017},
 title = {Attention Is All You Need},
 url = {https://arxiv.org/pdf/1706.03762.pdf},
 urldate = {09/05/2023},
 file = {1706.03762:Attachments/1706.03762.pdf:application/pdf}
}


% This file was created with Citavi 6.15.2.0

@article{Liu.2023,
 abstract = {Incorporating knowledge graphs into recommendation systems has attracted wide attention in various fields recently. A Knowledge graph contains abundant information with multi-type relations among multi-type nodes. The heterogeneous structure reveals not only the connectivity but also the complementarity between the nodes within a KG, which helps to capture the signal of potential interest of the user. However, existing research works have limited abilities in dealing with the heterogeneous nature of knowledge graphs, resulting in suboptimal recommendation results. In this paper, we propose a new recommendation method based on iterative heterogeneous graph learning on knowledge graphs (HGKR). By treating a knowledge graph as a heterogeneous graph, HGKR achieves more fine-grained modeling of knowledge graphs for recommendation. Specifically, we incorporate the graph neural networks into the message passing and aggregating of entities within a knowledge graph both at the graph and the semantic level. Furthermore, we designed a knowledge-perceiving item filter based on an attention mechanism to capture the user's potential interest in their historical preferences for the enhancement of recommendation. Extensive experiments conducted on two datasets in the context of two recommendations reveal the excellence of our proposed method, which outperforms other benchmark models.},
 author = {Liu, Tieyuan and Shen, Hongjie and Chang, Liang and Li, Long and Li, Jingjing},
 year = {2023},
 title = {Iterative heterogeneous graph learning for knowledge graph-based recommendation},
 pages = {6987},
 volume = {13},
 number = {1},
 journal = {Scientific reports},
 doi = {10.1038/s41598-023-33984-5}
}



@online{JessicaHoward.,
 author = {{Jessica Howard} },
 title = {Why is Product Taxonomy So Important for eCommerce},
 url = {https://datacluster.com/product-taxonomy-ecommerce},
 year ={2023}
}


% This file was created with Citavi 6.15.2.0

@misc{Tagliabue.26052020,
 abstract = {In an attempt to balance precision and recall in the search page, leading digital shops have been effectively nudging users into select category facets as early as in the type-ahead suggestions. In this work, we present SessionPath, a novel neural network model that improves facet suggestions on two counts: first, the model is able to leverage session embeddings to provide scalable personalization; second, SessionPath predicts facets by explicitly producing a probability distribution at each node in the taxonomy path. We benchmark SessionPath on two partnering shops against count-based and neural models, and show how business requirements and model behavior can be combined in a principled way.},
 author = {Tagliabue, Jacopo and Yu, Bingqing and Beaulieu, Marie},
 date = {26/05/2020},
 title = {How to Grow a (Product) Tree: Personalized Category Suggestions for  eCommerce Type-Ahead},
 url = {https://arxiv.org/pdf/2005.12781.pdf},
 urldate = {21/06/2023},
 file = {2005.12781:Attachments/2005.12781.pdf:application/pdf}
}


% This file was created with Citavi 6.15.2.0

@inproceedings{KarmakerSantu.2017,
 author = {{Karmaker Santu}, Shubhra Kanti and Sondhi, Parikshit and Zhai, ChengXiang},
 title = {On Application of Learning to Rank for E-Commerce Search},
 url = {https://arxiv.org/pdf/1903.04263.pdf},
 urldate = {09/12/2023},
 pages = {475--484},
 publisher = {ACM},
 isbn = {9781450350228},
 editor = {Kando, Noriko},
 booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 year = {2017},
 address = {New York, NY},
 doi = {10.1145/3077136.3080838},
 file = {On Application of Learning to Rank for E-Commerce Search:Attachments/On Application of Learning to Rank for E-Commerce Search.pdf:application/pdf}
}


@techreport{burges2010from,
author = {Burges, Chris J.C.},
title = {From RankNet to LambdaRank to LambdaMART: An Overview},
year = {2010},
month = {June},
abstract = {LambdaMART is the boosted tree version of LambdaRank, which is based on RankNet. RankNet, LambdaRank, and LambdaMART have proven to be very successful algorithms for solving real world ranking problems: for example an ensemble of LambdaMART rankers won Track 1 of the 2010 Yahoo! Learning To Rank Challenge. The details of these algorithms are spread across several papers and reports, and so here we give a self-contained, detailed and complete description of them.},
url = {https://www.microsoft.com/en-us/research/publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/},
number = {MSR-TR-2010-82},
}

@online{ChrisMessina,
  author = {Chris Messina},
  title = {Conversational commerce, Messaging apps bring the point of sale to you},
  url = {https://medium.com/chris-messina/conversational-commerce-92e0bccfc3ff},
  urldate = {Date Accessed},
  year = {2015}
}

