\chapter{Impute missing text data}

Machine leaning algorithms requires that their inputs have no missing values.
Missing values encoded as NaNs or blanks are incompatible with estimators which represents as the numerical values and assumes all values have and hold meaning. Discarding the entire row and/or columns of a dataset containing the missing values could lead to losing valuable data. The process to fill (impute) missing values are referred to as imputation.

\section{Feature imputation / Regression imputation or \\ Predictive Imputation}

Machine learning algorithm to predict values in a categorical variable based on other available features. Table \ref{table:feature_imputation} states the categorical features from the all the features mentioned in \ref{table:feature_decription}.


\begin{table}[h]
    \centering
    \caption{Identify categorical features}
    \label{table:feature_imputation}
    \begin{tabular}{ lll }
          \toprule
          
          \textbf{No}& \textbf{Feature} & \textbf{Categorical}\\
          \midrule
          1&Product name & No\\
          3&Description & No\\         
          4&Short description  & No\\
          5&Supplier  & Yes\\
          6&Manufacturer  &  Yes\\           
          7&Price  &  No \\
          8&Dimension  & Yes\\
          \bottomrule
          \end{tabular}
\end{table}

The \textit{dimension} column is also considered as a categorical value as it has few unique values.

If \textit{supplier} is the missing value from the set of row. The missing \textit{supplier} data must be predicted only from the available list of suppliers. A supervised machine learning with labeled dataset to train the model to classify the available features data to predict \textit{supplier}. A tensor size of the number of unique \textit{supplier} will be the output of the model. 
In an iterated round-robin fashion, at every step a missing feature column  \textit{y} and other feature columns treated as input \textit{x} predicts the missing values of \textit{y}. Regression imputation is more accurate than mode imputation on categorical value.


\section{Mode imputation}

Replacing the most frequent categorical value in a column is known as mode imputation.  Pandas.DataFrame.mode \parencite{mckinney-proc-scipy-2010} function returns most often value. Code snippet will fill the missing values for each column using its own most frequent value.

\begin{lstlisting}[language=Python, caption={Pandas DataFrames mode function}]
    df = df.fillna(df.mode().iloc[0])
\end{lstlisting}

Mode imputation on categorical data are prone to fill incorrect data if the missing data is not the most frequent one.



\section{\acf{KNN}  imputation}

\acl{KNN} is a supervised learning algorithm and is used to search dataset with the most similar elements to a given query element, with similarity defined by a distance function. This imputation method is suitable for categorical data.

The most common distance metrics functions are:-

\begin{enumerate}
    \item Euclidean distance.
    % \begin{lstlisting}[language=Python, caption={Euclidean distance formula }]
    %     dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))
    %         \end{lstlisting}
    \item Manhattan distance.
    \item Hamming distance.
\end{enumerate}

sklearn.neighbors.KNeighborsClassifier \parencite{scikit-learn} does an instance-based learning, it does not construct a model, but stores instance of training data. Classification is computed from majority vote of nearest neighbors of each point.

The \textit{k}-neighbors classification implements learning based on  \textit{k} nearest neighbors of each query point, where \textit{k} is integer value specified by user. The optimal \textit{k} value can be evaluated by iterating the classifier with different  \textit{n\textunderscore neighbors} parameter and finding the minimum value of error rate. As per \parencite{scikit-learn}, larger \textit{k} suppresses the effects of noise, but makes the classification boundaries less distinct.

\begin{lstlisting}[language=Python, caption={Find optimal \textit{k} value in \acl{KNN} }]
    error_rate = []
    for i in range(1,40):
        knn = KNeighborsClassifier(n_neighbors=i)
        knn.fit(X_train,y_train)
        pred_i = knn.predict(X_test)
        error_rate.append(np.mean(pred_i != y_test))

    print("Minimum error:-",min(error_rate),"at K =",error_rate.index(min(error_rate)))
\end{lstlisting}



\subsection{\acs{KNN} model implementation}

In table \ref{table:KNN_implementation} there are 7 columns. Target column is \textbf{Category-Level-3} which is the category at level three.
As per the example category hierarchy in table \ref{table:KNN_implementation} 



\begin{table}[h]
    \centering
    \caption{Sample features }
    \label{table:KNN_implementation}
    \begin{tabular}{ll}
        \toprule     
        \textbf{Name}& \textit{product name} \\
        \textbf{Description}& \textit{product description} \\
        \textbf{Short-Description}& \textit{product short description} \\
        \textbf{Category-Level-1}& Spare-Part \\
        \textbf{Category-Level-2}& Cooling System \\
        \textbf{Category-Level-3}& Thermostat \\
        \textbf{Category-Level-4}& NaN \textit{product has no level 4 category} \\
        \bottomrule
    \end{tabular}

\end{table}

\begin{itemize}
    \item Collect independent data features into the X data frame and target field into a y data frame.
    \item  Split data into training and testing. 
    \item  Import the classifier model from sklearn library and fit the model with  \textit{k} value equal to the optimal value.
\end{itemize}

\begin{lstlisting}[language=Python]
    X = df.drop(['catL3'], axis = 1)
    y = df['catL3']
    from sklearn import preprocessing
    X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn import metrics
    #Train Model and Predict
    k = 3  
    neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
    Pred_y = neigh.predict(X_test)
    print("Accuracy of model at K=3 is",metrics.accuracy_score(y_test, Pred_y))
\end{lstlisting}



\section{Mean/median imputation}

Pandas.DataFrame.median \parencite{mckinney-proc-scipy-2010} returns the median value. These can be applied to the features represented in numerical value. In reference to table \ref{table:feature_imputation}, \textit{price} could be a column on which median value can be filled. However, using median to fill missing value could underestimate or overestimate the value.

\begin{enumerate}
    \item Median - The mid point value
    \item Mean - The average value
\end{enumerate}

\begin{lstlisting}[language=Python]
    df = df.fillna(df.median().iloc[0])
\end{lstlisting}