\chapter*{Building knowledge graphs}


One of the first things required for natural language processing (NLP) tasks is a creating a corpus. In linguistics and NLP, corpus refers to a collection of texts \parencite{wkpd}.
A Wikipedia dump file is also required for this procedure the latest such English Wikipedia database dump file is ~14 GB in size.

You can export the text and editing history of a particular page or set of pages wrapped in XML. \cite{kdnug}
We exported XML file witg 


\includesvg{images/pronoun.svg}

