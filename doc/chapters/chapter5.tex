\chapter{Feature extraction}

Transforming text or image data into numerical representation usable for machine learning is called feature extraction. Raw sequence of data cannot be fed directly into a machine learning algorithm as they expect data in numerical vector and in fixed size. Whereas raw text document are of variable lengths. 

\section{Tokenization}

Tokenization is splitting the text in sequential words which can be embedded in a vector space.
For example, the normalized product name "abc joint kit drive shaft" can be tokenized into "abc", "joint", "kit", "drive", "shaft". 



\section{Counting}

Number of occurrence of each token in a document is called counting in terms of numerical feature extraction.

\section{Count Vectorization or One-Hot encoding} \label{ch_countvector}


CountVectorizer class of scikit learn API implements both tokenization and occurrence counting \parencite{sklearn_api}. 

Consider a data frame with columns \textit{ProductName} and data value as in tbale \ref{table:count_vectorization}

\begin{table}[h]
    \centering
    \caption{Sample data for count vectorization}
    \label{table:count_vectorization}
    \begin{tabular}{ l }
          \toprule
          
          \textbf{ProductName}\\
          \midrule
          abc joint kit drive shaft\\
          xyz joint kit drive shaft\\
         
          \bottomrule
          \end{tabular}
\end{table}

The length of the vocabulary is the number of unique tokens in the data frame column. In the sample data the vocabulary length is six. The vector has the dimensionality equal to the size of the vocabulary. Adding one in the dimension to each of the word in the vocabulary represents one hot encoding.

\begin{table}[]
    \centering
    \caption{Sample One-Hot encoding}
    \label{table:countencode}
    \begin{tabular}{ ll }
          \toprule
          
          \textbf{text}& \textbf{encoding}\\
          \midrule
          abc&[1,0,0,0,0,0]\\
          joint&[0,1,0,0,0,0]\\
          kit&[0,0,1,0,0,0]\\
          drive&[0,0,0,1,0,0]\\
          shaft&[0,0,0,0,1,0]\\
          xyz&[0,0,0,0,0,1]\\
       
          \bottomrule
          \end{tabular}
\end{table}

\section{ \textit{n-grams} Vectorization} \label{sec:ngram_vector}

Count Vectorization can also be performed on range of grams of words. 

\begin{lstlisting}[language=Python,label=ngramcode, caption={\textit{n-gram} vectorization}]
    bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))
    bigram_vectorizer.fit(df["ProductName"])
\end{lstlisting}


The code in listing \ref{ngramcode}, will add additional vocabularies of two words such as "drive shaft". This enables to preserve local ordering information. 

\section{Pickling the Vectorizer} \label{pickle_vector}

Pickle \footnote{https://docs.python.org/3/library/pickle.html} module creates a portable serialized representation of python object. Reusing the CountVectorizer object once it has been fit with the document can be enabled by Pickle module. 

\begin{lstlisting}[language=Python,label=pickle, caption={Pickle vectorization}]
    pickle.dump(self.vectorizer, open("vector.pickel", "wb"))
\end{lstlisting}