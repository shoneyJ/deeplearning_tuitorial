\chapter{Model selection : Neural networks}

\section{Understanding Pytorch tutorial on \textit{classifying names with a character-level \acs{RNN}} } \label{sec:chRNN}

\parencite{sean} tutorial on \textit{classifying names with a character-level \acs{RNN}} provides a basic foundation for classification algorithm. In this tutorial, \Citeauthor{sean} trains on few thousand surnames from 18 languages of origin, and predicts which language the name is from based on the spelling.
\subsection{One-Hot vector representation}
\Citeauthor{sean} uses one-hot vector of size 1 x no\textunderscore letters (26 letters). A one-hot vector is filled with 0s except for a 1 at index of the letter. For example, letter b is represented as 0,1,0,0...0. To make a word, author joins a bunch of letters into 2D matrix name\textunderscore length x 1 x no\textunderscore letters.

\begin{table}[h]
    \centering
    \caption{One-Hot vector representation of name James}
    \label{table:feature_imputation}
    \begin{tabular}{ lllllllllll }
          \toprule
          
          \textbf{Letter}& \textbf{a} & \textbf{...}& \textbf{e}&\textbf{...}&\textbf{j}&\textbf{...}&\textbf{m}&\textbf{...}&\textbf{s}&\textbf{...}\\
          \midrule
          J&0 & 0& 0& 0&1& 0& 0& 0& 0& 0\\
          a&1 & 0& 0& 0&0& 0& 0& 0& 0& 0\\         
          m&0 & 0& 0& 0&0& 0& 1& 0& 0& 0\\
          e&0 & 0& 1& 0&0& 0& 0& 0& 0& 0\\
          s&0 & 0& 0& 0&0& 0& 0& 0& 1& 0\\           
        
          \bottomrule
          \end{tabular}
\end{table}

\subsection{Vector input and scalar output of the classification model}
It is a character level \acs*{RNN} which reads words as a series of characters. Figure \ref{fig:pytorch} shows the flow of name information to the neural network as a sequence of characters. One character at a time is feed in classification model built using RNN and outputs the language corresponding to that name.

\begin{figure}[htp!]
    \centering    
    \includesvg[scale=0.5]{pytorchTuitorial.svg}
    \caption{Language prediction based on name}
    \label{fig:pytorch}
\end{figure}


\section{Ideate: Vocabulary level \acs{RNN}}

Inspired from the character level \acs{RNN} mentioned in section \ref{sec:chRNN}, author predicts category based on the name of the products by creating a vocabulary level \acs{RNN}. As described in section \ref{ch_countvector}, product names are converted into one-hot encoded format. Vector representation of vocabulary in product name across the data frame is created. These encoded pattern of name serves as an input to the machine learning model. Model learns these patterns and predicts the category in which these patterns belong to. In section \ref{nametotensor}, the input tensors are modified to fit for use case of predicting category based on product name.  

\subsection{Convert product name to tensors} \label{nametotensor}

\begin{lstlisting}[language=Python,label=productnametotensor, caption={Convert product name to tensors}]
    def nameToTensor(self,name):
        vectorizer= pickle.load(open("vector.pickel", "rb"))
        inputSize=len(vectorizer.vocabulary_)
        vectorized=vectorizer.transform(list(name.split()))
        name_tensor=torch.zeros(1, inputSize)
        for index in vectorized.indices:
            name_tensor[0][index] = 1
        
        return name_tensor
\end{lstlisting}

\begin{enumerate}
    \item vectorizer: \\
    In section \ref{pickle_vector}, using the Pickle module the vector object had been stored. The object contains the vector representation of vocabulary of product names. Code line number 2 loads the object and stores the value in vectorizer variable.
    \item vector size: \\
    Code line number 3 gets the length of the vector vocabulary. The number of unique words in the product name across entire column of \textit{ProductName}
    \item transform : \\
    As described in section \ref{sec:ngram_vector}, based on the existing vocabulary, vectorizer object's transform function returns token counts out of raw text documents using the vocabulary fitted with fit method or the array of tokens into the one hot encoded vectorized form. 
    \item torch.zero \footnote{https://pytorch.org/docs/stable/generated/torch.zeros.html}: \\
    Initialize a tensor variable filled with scalar value 0.

    \item Set 1 for each vectorized index.
    
\end{enumerate}

For example, consider a vocabulary of 100 unique words in the \textit{ProductName} dataset. The name of the product is \textit{abc joint kit drive shaft}. Assuming the index value of each of the token are as per table \ref{table:names index} then the tensor value of the product will be as per table \ref{table:tensorvalue} 

\begin{table}[htp!]
    \centering
    \caption{Example: Index value of product name}
    \label{table:names index}
    \begin{tabular}{ ll }
          \toprule
          
          \textbf{Token} & \textbf{Index}\\
          \midrule
          abc&0\\
          joint&10\\
          kit&26\\
          drive&78\\
          shaft&99\\
                 
        
          \bottomrule
          \end{tabular}
\end{table}

\begin{table}[htp!]
    \centering
    \caption{Tensor value of example product name}
    \label{table:tensorvalue}
    \begin{tabular}{ llllllllll }
          \toprule
          
          \textbf{0} & \textbf{...}& \textbf{10}&\textbf{...}&\textbf{26}&\textbf{...}&\textbf{78}&\textbf{...}&\textbf{99}\\
          \midrule
          1 & 0& 1& 0&1& 0& 1 & 0& 1\\
                 
        
          \bottomrule
          \end{tabular}
\end{table}

% \section{\Citeauthor{sean} input tensors vs Product name to tensor}


% \begin{lstlisting}[language=Python,label=sean, caption={\Citeauthor{sean} input tensor}]
%     def lineToTensor(line):
%         tensor = torch.zeros(len(line), 1, n_letters)
%         for li, letter in enumerate(line):
%             tensor[li][0][letterToIndex(letter)] = 1
%         return tensor
% \end{lstlisting}

\section{Architecture of \acf{RNN}}
\acs{RNN} module copied from \parencite{sean} tutorial contains two linear layers which operate on input and hidden state, with LogSoftmax layer after the output layer. \acfp{RNN} \parencite{Rumelhart.1986} is best suited for machine learning problems that involve sequential data and use patterns to predict the output. This machine learning algorithm store information of previous states and holds the memory. 
\begin{lstlisting}[language=Python,label=code:RNN-class, caption={\acf{RNN} class}]
class RNN(nn.Module):
        
    def __init__(self, input_size, hidden_size, output_size):
        
        super(RNN, self).__init__()

        self.hidden_size = hidden_size

        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        
        self.h2o = nn.Linear(hidden_size, output_size)
        
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, current_input, previous_hidden):
        compbined = torch.cat((current_input, previous_hidden), 1)
        
        next_hidden = self.i2h(compbined)
        
        output = self.h2o(next_hidden)
        output = self.softmax(output)
        
        return output, next_hidden

    def initHidden(self):
        return torch.zeros(1, self.hidden_size)
\end{lstlisting}

The class RNN represents the RNN model with an input layer, hidden layer, and output layer. It uses the nn.Linear and nn.LogSoftmax functions from PyTorch's neural network module.

\textbf{Summary of the RNN class:}

\textit{Constructor (\_\_init\_\_ method):} Initializes the RNN with the input size, hidden size, and output size. It creates the layers and initializes the hidden size. The \texttt{nn.Linear} objects \texttt{i2h} and \texttt{h2o} represent the weight matrices for the input-to-hidden and hidden-to-output transformations, respectively. The \texttt{nn.LogSoftmax} object \texttt{softmax} applies the logarithm and softmax operations to convert the output into log probabilities.

\textit{Forward pass (\texttt{forward} method):} Takes the current input and previous hidden state as input and computes the forward pass of the RNN. It concatenates the current input and previous hidden state using \texttt{torch.cat}, passes the concatenated tensor through the input-to-hidden layer, calculates the output using the hidden-to-output layer, and applies the softmax operation to obtain the normalized log probabilities.

\textit{Initialization of hidden state (\texttt{initHidden} method):} Returns the initial hidden state, which is a tensor of zeros with dimensions (1, hidden\_size).

\clearpage
The graphical representation of code listed in \ref{code:RNN-class} is illustrated in figure \ref{fig:archi}.
\begin{figure}[h]
    \centering
    \caption{\acs{RNN} Architecture \parencite{sean}}
    \label{fig:archi}
    \begin{tikzpicture}[
        roundnode/.style={circle, draw=green!60, fill=green!5, very thick, minimum width={width("hidden")},}
        ]
    
        % Nodes
        \node[roundnode] (combined) {Combined};
        \coordinate[above of=combined] (ac);
        \coordinate[below of=combined] (bc);
        \node[roundnode] (input) [left=2cm of ac] {Input};
        \node[roundnode] (hidden) [right=2cm of ac] {Hidden};
        \node[roundnode] (i2o) [below=2cm of input] {In2Out};
        \node[roundnode] (i2h) [below=2cm of hidden] {In2Hid};
        \node[roundnode] (softmax) [below=1cm of i2o] {Softmax};
        \node[roundnode] (12hidden) [below=4cm of i2h] {Hidden};
        \node[roundnode] (output) [below=of softmax] {Output};
            
        % Lines
        \draw[->] (input.south) .. controls +(down:7mm) and +(left:7mm) ..  (combined.west);
        \draw[->] (hidden.south) .. controls +(down:7mm) and +(right:7mm) ..  (combined.east);
        \draw[->] (combined.south) .. controls +(down:7mm) and +(right:7mm) ..   (i2o.east);
        \draw[->] (combined.south).. controls +(down:7mm) and +(left:7mm) .. (i2h.west);
        \draw[->] (i2o.south) --  (softmax.north);
        \draw[->] (i2h.south) --  (12hidden.north);
        \draw[->] (softmax.south) --  (output.north);
        
        \draw[->] (12hidden.east) .. controls +(up:1cm) and +(right:4cm) ..  (hidden.east);
    
    \end{tikzpicture}

\end{figure}
\begin{itemize}
    \item input\textunderscore size : Input parameter of class is size of the data. Size of one-hot encoded feature vector. 
    \item  hidden\textunderscore size : Dimensionality of hidden state of the \acs{RNN} cell.
    \item  output\textunderscore size : Number of categories in which the input need to be classified.
    \item i2h : input-to-hidden, a fully connected linear layer gets the next hidden state from the current input and previous state.
    \item i2o : input-to-output,a fully connected linear layer gets the next output state from the current input and previous hidden state.
    \item softmax: Layer used for classification. 
\end{itemize}

\section{Forward propagation equation}
The forward propagation equations for the recurrent neural network (RNN) depicted in code \ref{code:RNN-class}, assuming the tanh activation function for hidden units and a discrete output (used for predicting words or characters) \parencite{Goodfellow-et-al-2016}, are as follows
\begin{enumerate}
    \item Initialization: \\
    Set initial state: $\textit{\textbf{h}}^{(0)}$ (hidden state at time $t=0$)
    \item For each time step from $t=1$ to $t=\tau$: \\
    \begin{align}
        a(t) &= b + Wh(t-1) + Ux(t) \quad  \\
        h(t) &= \tanh(a(t)) \quad  \\
        o(t) &= c + Vh(t) \quad  \\
        \hat{y}(t) &= \text{softmax}(o(t)) \quad 
        \end{align}
\end{enumerate}

Here:
\begin{itemize}
  \item $x(t)$ represents the input at time step $t$.
  \item $h(t)$ is the hidden state at time step $t$, which is updated based on the previous hidden state $h(t-1)$ and the current input $x(t)$.
  \item $a(t)$ is the activation vector at time step $t$, computed by adding the bias vector $b$ to the weighted sum of the previous hidden state $h(t-1)$ and the current input $x(t)$, where $W$ is the weight matrix for the recurrent connections and $U$ is the weight matrix for the input connections.
  \item $\tanh()$ is the hyperbolic tangent activation function, which squashes the values between $-1$ and $1$, introducing non-linearity to the hidden state.
  \item $o(t)$ is the output vector at time step $t$, computed by adding the bias vector $c$ to the weighted sum of the current hidden state $h(t)$ using the weight matrix $V$.
  \item $\hat{y}(t)$ is the normalized probability vector (predicted probabilities) over the output, obtained by applying the softmax operation to $o(t)$, which converts the unnormalized log probabilities into valid probabilities summing up to $1$.
\end{itemize}



\section{Pytorch's nn.Softmax function and applying log }

Pytorch's nn.LogSoftmax function applies $\text{log}\left(\text{Softmax}(\mathbf{x})\right)$ to an n-dimentional input tensors. \\



\subsection{Softmax or normalized exponential \parencite{Book-Bishop-Neural}}

Pytorch's nn.Softmax applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.

\begin{table}[h]
    \centering
    \begin{tabular}{ccccccc}
        \hline
        $X$ & $X_1$ & $X_2$ & $X_3$ & $\dots$ & $X_n$ \\
        \hline
        $P(X)$ & $P_1$ & $P_2$ & $P_3$ & $\dots$ & $P_n$ \\
        \hline
    \end{tabular}
    \caption{Probability Distribution $P(X)$ for Random Variable $X$}
    \label{tab:probability-distribution}
\end{table}

where \(P_i > 0\), \(i=1\) to \(n\) and \(P_1 + P_2 + P_3 + \ldots + P_n = 1\) \\


\begin{align}
    \text{Softmax}(x_i) = \frac{{\exp(x_i)}}{{\sum_{j=1}^n \exp(x_j)}} \label{eq:softmax}
\end{align}


In this representation, \(x_i\) is the \(i\)-th element of the input vector \(\mathbf{x} = [x_1, x_2, \ldots, x_n]\). The softmax function calculates the probability of the element \(x_i\) being chosen by dividing the exponential of \(x_i\) by the sum of the exponentials of all elements in the vector \(\mathbf{x}\). This calculation ensures that the resulting probabilities sum up to 1, forming a valid probability distribution over the elements in the vector \(\mathbf{x}\).

The softmax function effectively transforms a vector of logits into a probability distribution, making it useful for tasks like classification, where the model needs to make a decision based on the input data.

As per \parencite{Book-Bishop-Neural} the term softmax is used because this activation function represents a smooth version of the winner-takes-all activation model in which the unit with the largest input has output +1 while all other units have output 0.

\clearpage

\section{Why to apply log to Softmax Equation \eqref{eq:softmax} }

 

\begin{align}
    \log(\text{Softmax}(x_i)) = \log\left(\frac{{\exp(x_i)}}{{\sum_{j=1}^n \exp(x_j)}}\right) \label{eq:log_softmax}
\end{align}

Equation \eqref{eq:log_softmax} shows the logarithm of the Softmax function applied to the element \(x_i\) in the vector \(\mathbf{x}\). 

Applying logarithm to the fraction: 
\begin{align}
    \log\left(\frac{a}{b}\right) = \log(a) - \log(b)
\end{align}

Substituting \(a = \exp(x_i)\) and \(b = \sum_{j=1}^n \exp(x_j)\) in Equation \eqref{eq:log_softmax}, we get:

\begin{align}
    \log(\text{Softmax}(x_i)) = \log(\exp(x_i)) - \log\left(\sum_{j=1}^n \exp(x_j)\right)
\end{align}

1. Reduce large numbers:  The softmax function normalizes any value from [-infy, +infy] by applying an exponential function. Exponential functions can lead to a very large numbers, resulting in  Nan as output. Applying logarithm effectively reduce these large numbers to manageable range.
2. Logarithms and Multiplication: Logarithms convert multiplication to addition. Probabilities of independent events,  represented as \(P(A \cap B \cap C \cap \ldots)\). Logarithm of the product of these probabilities simplifies the computation to a sum. 

3. Numerical Stability: When dealing with very small probabilities, the product of multiple probabilities can become exceedingly close to zero, leading to numerical underflow. Similarly, when dealing with very large probabilities, the product can exceed the representation range of standard floating-point numbers, causing numerical overflow. By taking the logarithm, these issues are avoided, and the values are represented in a more stable and manageable range.

4. Log Probabilities and Addition: Log probabilities are additive when events are independent.\\ For example, if we have two independent events \(A\) and \(B\) with probabilities \(P(A) = p\) and \(P(B) = q\), the probability of both events occurring (\(P(A \cap B)\)) is \(P(A) \times P(B) = p \times q\). Taking the logarithm, we get \(\log(P(A \cap B)) = \log(p \times q) = \log(p) + \log(q)\). This additive property simplifies calculations involving multiple independent events.

In summary, using log-softmax over softmax is the use of log probabilities over probabilities, a log probability is simply a logarithm of a probability. The use of log probabilities means representing probabilities on a logarithmic scale. Working with log probabilities allows for more stable and efficient computations, especially when dealing with products of probabilities of very small or large numbers. 