\chapter{Defining Model Architecture: Neural networks} \label{sec:feature-extraction}

\section{Understanding Pytorch tutorial on \textit{classifying names with a character-level \acs{RNN}} } \label{sec:chRNN}

\parencite{sean} tutorial on \textit{classifying names with a character-level \acs{RNN}} provides a basic foundation for classification algorithm. In this tutorial, \Citeauthor{sean} trains on few thousand surnames from 18 languages of origin, and predicts which language the name is from based on the spelling.
\subsection{One-Hot vector representation}
\Citeauthor{sean} uses one-hot vector of size 1 x no\textunderscore letters (26 letters). A one-hot vector is filled with 0s except for a 1 at index of the letter. For example, letter b is represented as 0,1,0,0...0. To make a word, author joins a bunch of letters into 2D matrix name\textunderscore length x 1 x no\textunderscore letters.

\begin{table}[h]
    \centering
    \caption{One-Hot vector representation of name James}
    \label{table:feature_imputation}
    \begin{tabular}{ lllllllllll }
          \toprule
          
          \textbf{Letter}& \textbf{a} & \textbf{...}& \textbf{e}&\textbf{...}&\textbf{j}&\textbf{...}&\textbf{m}&\textbf{...}&\textbf{s}&\textbf{...}\\
          \midrule
          J&0 & 0& 0& 0&1& 0& 0& 0& 0& 0\\
          a&1 & 0& 0& 0&0& 0& 0& 0& 0& 0\\         
          m&0 & 0& 0& 0&0& 0& 1& 0& 0& 0\\
          e&0 & 0& 1& 0&0& 0& 0& 0& 0& 0\\
          s&0 & 0& 0& 0&0& 0& 0& 0& 1& 0\\           
        
          \bottomrule
          \end{tabular}
\end{table}
\clearpage
\subsection{Vector input and scalar output of the classification model}
It is a character level \acs*{RNN} which reads words as a series of characters. Figure \ref{fig:pytorch} shows the flow of name information to the neural network as a sequence of characters. One character at a time is feed in classification model built using RNN and outputs the language corresponding to that name.

\begin{figure}[H]
    \centering    
    \includesvg[scale=0.5]{pytorchTuitorial.svg}
    \caption{Language prediction based on name}
    \label{fig:pytorch}
\end{figure}


\section{Ideate: Vocabulary level \acs{RNN}} \label{sec:ideate}

Inspired from the character level \acs{RNN} mentioned in section \ref{sec:chRNN}, author predicts category based on the name of the products by creating a vocabulary level \acs{RNN}. As described in section \ref{ch_countvector}, product names are converted into one-hot encoded format. Vector representation of vocabulary in product name across the data frame is created. These encoded pattern of name serves as an input to the machine learning model. Model learns these patterns and predicts the category in which these patterns belong to. In section \ref{nametotensor}, the input tensors are modified to fit for use case of predicting category based on product name.  

\subsection{Convert product name to tensors} \label{nametotensor}

\begin{lstlisting}[language=Python,label=productnametotensor, caption={Convert product name to tensors},label={cd:pt}]
    def nameToTensor(self,name):
        vectorizer= pickle.load(open("vector.pickel", "rb"))
        inputSize=len(vectorizer.vocabulary_)
        vectorized=vectorizer.transform(list(name.split()))
        name_tensor=torch.zeros(1, inputSize)
        for index in vectorized.indices:
            name_tensor[0][index] = 1
        
        return name_tensor
\end{lstlisting}

Summary of code snippet \ref{cd:pt}:-

\begin{enumerate}
    \item vectorizer: \\
    In section \ref{pickle_vector}, author describes using Pickle module to store vector object. The object contains the vector representation of vocabulary of product names. Code line number 2 loads the object and stores the value in vectorizer variable.
    \item vector size: \\
    Code line number 3 gets the length of the vector vocabulary. The number of unique words in the product name across entire column of \textit{ProductName}
    \item transform : \\
    As described in section \ref{sec:ngram_vector}, based on the existing vocabulary, vectorizer object's transform function returns token counts out of raw text documents using the vocabulary fitted with fit method or the array of tokens into the one hot encoded vectorized form. 
    \item torch.zero \footnote{https://pytorch.org/docs/stable/generated/torch.zeros.html}: \\
    Initialize a tensor variable filled with scalar value 0.

    \item Set 1 for each vectorized index.
    
\end{enumerate}

For example, consider a vocabulary of 100 unique words in the \textit{ProductName} dataset. The name of the product is \textit{abc joint kit drive shaft}. Assuming the index value of each of the token are as per table \ref{table:names index} then the tensor value of the product will be as per table \ref{table:tensorvalue} 

\begin{table}[htp!]
    \centering
    \caption{Example: Index value of product name}
    \label{table:names index}
    \begin{tabular}{ ll }
          \toprule
          
          \textbf{Token} & \textbf{Index}\\
          \midrule
          abc&0\\
          joint&10\\
          kit&26\\
          drive&78\\
          shaft&99\\
                 
        
          \bottomrule
          \end{tabular}
\end{table}

\begin{table}[htp!]
    \centering
    \caption{Tensor value of example product name}
    \label{table:tensorvalue}
    \begin{tabular}{ llllllllll }
          \toprule
          
          \textbf{0} & \textbf{...}& \textbf{10}&\textbf{...}&\textbf{26}&\textbf{...}&\textbf{78}&\textbf{...}&\textbf{99}\\
          \midrule
          1 & 0& 1& 0&1& 0& 1 & 0& 1\\
                 
        
          \bottomrule
          \end{tabular}
\end{table}

\section{Architecture of \acf{RNN}}
\acs{RNN} architecture is from \parencite{sean} tutorial, this module contains two linear layers which operate on input and hidden state, with $LogSoftmax$ layer after the output layer. The class RNN represents the RNN model with an input layer, hidden layer, and output layer. It uses the nn.Linear and nn.LogSoftmax functions from PyTorch's neural network module.  The code listing \ref{code:rnn_arch} is a snippet of python code to define class named RNN.

\begin{lstlisting}[language=Python,label=code:RNN-class, caption={\acf{RNN} class}, label={code:rnn_arch}]
class RNN(nn.Module):
        
    def __init__(self, input_size, hidden_size, output_size):
        
        super(RNN, self).__init__()

        self.hidden_size = hidden_size

        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        
        self.h2o = nn.Linear(hidden_size, output_size)
        
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, current_input, previous_hidden):
        compbined = torch.cat((current_input, previous_hidden), 1)
        
        next_hidden = self.i2h(compbined)
        
        output = self.h2o(next_hidden)
        output = self.softmax(output)
        
        return output, next_hidden

    def initHidden(self):
        return torch.zeros(1, self.hidden_size)
\end{lstlisting}

\clearpage
\subsection*{Summary of the RNN class:}

\begin{itemize}
    \item \textit{Constructor (\_\_init\_\_ method):} \\ Initializes the RNN with the input size, hidden size, and output size. It creates the layers and initializes the hidden size. The \texttt{nn.Linear} objects \texttt{i2h} and \texttt{h2o} represent the weight matrices for the input-to-hidden and hidden-to-output transformations, respectively. The \texttt{nn.LogSoftmax} function applies the logarithm and softmax operations to convert the output into log probabilities.
    \item \textit{Forward pass (\texttt{forward} method):} Takes the current input and previous hidden state as input and computes the forward pass of the RNN. It concatenates the current input and previous hidden state using \texttt{torch.cat}, passes the concatenated tensor through the input-to-hidden layer, calculates the output using the hidden-to-output layer, and applies the $LogSoftmax$ operation to obtain the normalized log probabilities.
    \item \textit{Initialization of hidden state (\texttt{initHidden} method):} Returns the initial hidden state, which is a tensor of zeros with dimensions (1, hidden\_size).
\end{itemize}







\clearpage
The graphical representation of code listed in \ref{code:rnn_arch} is illustrated in figure \ref{fig:archi}.
\begin{figure}
    \centering
    \caption{\acs{RNN} Architecture \parencite{sean}}
    \label{fig:archi}
    \begin{tikzpicture}[
        roundnode/.style={circle, draw=green!60, fill=green!5, very thick, minimum width={width("hidden")},}
        ]
    
        % Nodes
        \node[roundnode] (combined) {Combined};
        \coordinate[above of=combined] (ac);
        \coordinate[below of=combined] (bc);
        \node[roundnode] (input) [left=2cm of ac] {Input};
        \node[roundnode] (hidden) [right=2cm of ac] {Hidden};
        \node[roundnode] (i2o) [below=2cm of input] {In2Out};
        \node[roundnode] (i2h) [below=2cm of hidden] {In2Hid};
        \node[roundnode] (softmax) [below=1cm of i2o] {Softmax};
        \node[roundnode] (12hidden) [below=4cm of i2h] {Hidden};
        \node[roundnode] (output) [below=of softmax] {Output};
            
        % Lines
        \draw[->] (input.south) .. controls +(down:7mm) and +(left:7mm) ..  (combined.west);
        \draw[->] (hidden.south) .. controls +(down:7mm) and +(right:7mm) ..  (combined.east);
        \draw[->] (combined.south) .. controls +(down:7mm) and +(right:7mm) ..   (i2o.east);
        \draw[->] (combined.south).. controls +(down:7mm) and +(left:7mm) .. (i2h.west);
        \draw[->] (i2o.south) --  (softmax.north);
        \draw[->] (i2h.south) --  (12hidden.north);
        \draw[->] (softmax.south) --  (output.north);
        
        \draw[->] (12hidden.east) .. controls +(up:1cm) and +(right:4cm) ..  (hidden.east);
    
    \end{tikzpicture}

\end{figure}
\begin{itemize}
    \item input\textunderscore size : Input parameter of class is size of the data. Size of one-hot encoded feature vector. 
    \item  hidden\textunderscore size : Dimensionality of hidden state of the \acs{RNN} cell.
    \item  output\textunderscore size : Number of categories in which the input need to be classified.
    \item i2h : input-to-hidden, a fully connected linear layer gets the next hidden state from the current input and previous state.
    \item i2o : input-to-output,a fully connected linear layer gets the next output state from the current input and previous hidden state.
    \item softmax: Layer used for classification. 
\end{itemize}

  

% \section{Forward propagation equation} \label{sec:forward-propagation}
% The forward propagation equations for the recurrent neural network (RNN) depicted in code \ref{code:RNN-class}, assuming the tanh activation function for hidden units and a discrete output (used for predicting words or characters) \parencite{Goodfellow-et-al-2016}, are as follows
% \begin{enumerate}
%     \item Initialization: \\
%     Set initial state: $\textit{\textbf{h}}^{(0)}$ (hidden state at time $t=0$)
%     \item For each time step from $t=1$ to $t=\tau$: \\
%     \begin{align}
%         a(t) &= b + Wh(t-1) + Ux(t) \quad  \\
%         h(t) &= \tanh(a(t)) \quad  \\
%         o(t) &= c + Vh(t) \quad  \\
%         \hat{y}(t) &= \text{softmax}(o(t)) \quad 
%         \end{align}
% \end{enumerate}

% Here:
% \begin{itemize}
%   \item $x(t)$ represents the input at time step $t$.
%   \item $h(t)$ is the hidden state at time step $t$, which is updated based on the previous hidden state $h(t-1)$ and the current input $x(t)$.
%   \item $a(t)$ is the activation vector at time step $t$, computed by adding the bias vector $b$ to the weighted sum of the previous hidden state $h(t-1)$ and the current input $x(t)$, where $W$ is the weight matrix for the recurrent connections and $U$ is the weight matrix for the input connections.
%   \item $\tanh()$ is the hyperbolic tangent activation function, which squashes the values between $-1$ and $1$, introducing non-linearity to the hidden state.
%   \item $o(t)$ is the output vector at time step $t$, computed by adding the bias vector $c$ to the weighted sum of the current hidden state $h(t)$ using the weight matrix $V$.
%   \item $\hat{y}(t)$ is the normalized probability vector (predicted probabilities) over the output, obtained by applying the softmax operation to $o(t)$, which converts the unnormalized log probabilities into valid probabilities summing up to $1$.
% \end{itemize}

\clearpage

\section{Pytorch's $SoftMax$ function and its variations}

\subsection*{Softmax or normalized exponential.} \label{sec:softmax}

\begin{align}
    \text{Softmax}(x_i) = \frac{{\exp(x_i)}}{{\sum_{j=1}^n \exp(x_j)}} \label{eq:softmax}
\end{align}

\begin{itemize}
    \item Pytorch's nn.Softmax applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.
    \item In this representation, \(x_i\) is the \(i\)-th element of the input vector \(\mathbf{x} = [x_1, x_2, \ldots, x_n]\). The softmax function calculates the probability of the element \(x_i\) being chosen by dividing the exponential of \(x_i\) by the sum of the exponentials of all elements in the vector \(\mathbf{x}\). This calculation ensures that the resulting probabilities sum up to 1, forming a valid probability distribution over the elements in the vector \(\mathbf{x}\).
    \item The softmax function transforms a vector of logits into a probability distribution, making it useful for tasks like classification, where the model needs to make a decision based on the input data.
    \item Pytorch's nn.LogSoftmax function applies $\text{log}\left(\text{Softmax}(\mathbf{x})\right)$ to an n-dimentional input tensors. \\
\end{itemize}



% As per \parencite{Book-Bishop-Neural} the term softmax is used because this activation function represents a smooth version of the winner-takes-all activation model in which the unit with the largest input has output +1 while all other units have output 0.



\subsection*{\acf{ALL}}

\begin{itemize}
    \item Pytorch's \parencite{Paszke.03122019} nn.AdaptiveLogSoftmaxWithLoss module implements \acf{ALL}.
    \item Adaptive Softmax \parencite{Grave.14092016} is effective when large number of classes are to be handled. Traditional Softmax computation becomes computationally expensive and memory intensive if number of output classes are very high. Softmax requires calculating the exponential of the logits for each class. However, when the number of classes or labels are very high, the Softmax based classification becomes computationally expensive.
    \item Adaptive Softmax counter this issue by clustering the classes according to their frequency of occurrence. A simple strategy to reduce the overall computation time is to partition the labels \(V\) into two clusters as \(V_h\) and \(V_l\), where \(V_h\) denotes the distribution consisting of the most frequent classes, and where \(V_l\) are many rare classes. The classifier frequently accesses \(V_h\), which motivates the fact that it should be computed efficiently. In contrast,  \(V_l\) less frequently, and the corresponding computation can be slower. This suggests defining clusters with unbalanced cardinalities \(|V_h| \geq |V_l|\) and probabilities \(P(V_h) \geq P(V_t)\).
\end{itemize}










\section{Summary}

In this chapter, author describes how he ideated the concept of vocabulary based RNN. This method is a modified version of character level RNN \parencite{sean}. The method of converting the product name into a tensor format is described in this chapter. 

Architecture of the RNN and pictorial representation of flow of data within the neural network is depicted. The chapter gives an overview of the activation functions and forward propagation method. The code snippet of the defining architecture of \acl{RNN} class is given along with its explaination.  

In this chapter, Pytorch's loss functions such as  SoftMax, LogSoftMax, Adaptive Softmax and its equivalent mathematical  representations is described. 